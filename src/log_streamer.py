"""
Spark Structured Streaming pour ing√©rer des nouveaux logs en temps r√©el
Simule l'arriv√©e de nouveaux logs toutes les 10 secondes
"""

from pyspark.sql import SparkSession, DataFrame
from pyspark.sql.functions import col, window, count, current_timestamp
from pyspark.sql.streaming.query import StreamingQuery
import time
import os
from src.log_parser import LogParser

class LogStreamer:
    """Streaming de logs en temps r√©el avec Spark Structured Streaming"""
    
    def __init__(self, spark: SparkSession):
        self.spark = spark
        self.parser = LogParser(spark)
    
    def create_streaming_source(self, source_dir: str) -> DataFrame:
        """
        Cr√©er une source de streaming qui lit les nouveaux fichiers
        dans un r√©pertoire
        
        Args:
            source_dir: R√©pertoire contenant les fichiers de logs
        
        Returns:
            DataFrame en streaming
        """
        print(f"\nüåä CR√âATION DE LA SOURCE STREAMING")
        print(f"R√©pertoire surveill√©: {source_dir}")
        print("=" * 100)
        
        # Lire les fichiers texte en mode streaming
        streaming_df = self.spark.readStream \
            .format("text") \
            .option("maxFilesPerTrigger", 1) \
            .load(source_dir)
        
        return streaming_df
    
    def process_streaming_logs(self, streaming_df: DataFrame) -> DataFrame:
        """
        Parser et traiter les logs en streaming
        
        Args:
            streaming_df: DataFrame en streaming (texte brut)
        
        Returns:
            DataFrame en streaming avec logs pars√©s
        """
        from pyspark.sql.functions import (
            regexp_extract, to_timestamp, when, 
            hour, dayofweek, date_format
        )
        from pyspark.sql.types import IntegerType, LongType
        from src.log_parser import APACHE_LOG_PATTERN
        
        # Parser avec regex (m√™me logique que LogParser mais en streaming)
        logs_df = streaming_df.select(
            regexp_extract('value', APACHE_LOG_PATTERN, 1).alias('ip'),
            regexp_extract('value', APACHE_LOG_PATTERN, 4).alias('timestamp_str'),
            regexp_extract('value', APACHE_LOG_PATTERN, 5).alias('method'),
            regexp_extract('value', APACHE_LOG_PATTERN, 6).alias('url'),
            regexp_extract('value', APACHE_LOG_PATTERN, 7).alias('protocol'),
            regexp_extract('value', APACHE_LOG_PATTERN, 8).alias('status'),
            regexp_extract('value', APACHE_LOG_PATTERN, 9).alias('bytes_str')
        )
        
        # Nettoyer et convertir
        logs_cleaned = logs_df.filter(
            (col('ip') != '') & (col('timestamp_str') != '')
        ).select(
            col('ip'),
            to_timestamp(col('timestamp_str'), 'dd/MMM/yyyy:HH:mm:ss Z').alias('timestamp'),
            col('method'),
            col('url'),
            col('protocol'),
            when(col('status') != '-', col('status').cast(IntegerType()))
            .otherwise(None).alias('status'),
            when(col('bytes_str') != '-', col('bytes_str').cast(LongType()))
            .otherwise(0).alias('bytes')
        ).withColumn(
            'processing_time', current_timestamp()
        ).withColumn(
            'hour', hour('timestamp')
        ).withColumn(
            'is_error', when(col('status') >= 400, 1).otherwise(0)
        )
        
        return logs_cleaned
    
    def aggregate_streaming_metrics(self, streaming_df: DataFrame) -> DataFrame:
        """
        Agr√©ger des m√©triques en temps r√©el sur des fen√™tres de temps
        
        Args:
            streaming_df: DataFrame en streaming avec logs pars√©s
        
        Returns:
            DataFrame avec m√©triques agr√©g√©es
        """
        from pyspark.sql.functions import sum as _sum, approx_count_distinct
        
        # Agr√©gation par fen√™tre de 1 minute
        metrics = streaming_df \
            .withWatermark("timestamp", "10 minutes") \
            .groupBy(
                window("timestamp", "1 minute", "10 seconds")
            ).agg(
                count("*").alias("request_count"),
                approx_count_distinct("ip").alias("unique_ips"),
                _sum("bytes").alias("total_bytes"),
                _sum("is_error").alias("error_count")
            )
        
        return metrics
    
    def write_to_console(self, streaming_df: DataFrame, 
                         checkpoint_dir: str,
                         query_name: str = "console_output") -> StreamingQuery:
        """
        √âcrire les r√©sultats du streaming vers la console
        
        Args:
            streaming_df: DataFrame en streaming
            checkpoint_dir: R√©pertoire pour les checkpoints
            query_name: Nom de la query
        
        Returns:
            StreamingQuery
        """
        query = streaming_df.writeStream \
            .outputMode("append") \
            .format("console") \
            .option("truncate", "false") \
            .option("checkpointLocation", f"{checkpoint_dir}/{query_name}") \
            .queryName(query_name) \
            .trigger(processingTime="10 seconds") \
            .start()
        
        print(f"\n‚úÖ Query '{query_name}' d√©marr√©e")
        print(f"Checkpoint: {checkpoint_dir}/{query_name}")
        
        return query
    
    def write_to_parquet(self, streaming_df: DataFrame,
                         output_path: str,
                         checkpoint_dir: str,
                         query_name: str = "parquet_output") -> StreamingQuery:
        """
        √âcrire les r√©sultats du streaming en format Parquet
        
        Args:
            streaming_df: DataFrame en streaming
            output_path: Chemin de sortie pour les fichiers Parquet
            checkpoint_dir: R√©pertoire pour les checkpoints
            query_name: Nom de la query
        
        Returns:
            StreamingQuery
        """
        query = streaming_df.writeStream \
            .outputMode("append") \
            .format("parquet") \
            .option("path", output_path) \
            .option("checkpointLocation", f"{checkpoint_dir}/{query_name}") \
            .queryName(query_name) \
            .trigger(processingTime="10 seconds") \
            .start()
        
        print(f"\n‚úÖ Query '{query_name}' d√©marr√©e")
        print(f"Output: {output_path}")
        print(f"Checkpoint: {checkpoint_dir}/{query_name}")
        
        return query
    
    def write_to_memory(self, streaming_df: DataFrame,
                       query_name: str = "memory_table") -> StreamingQuery:
        """
        √âcrire les r√©sultats en m√©moire (table temporaire)
        Utile pour les requ√™tes interactives
        
        Args:
            streaming_df: DataFrame en streaming
            query_name: Nom de la table en m√©moire
        
        Returns:
            StreamingQuery
        """
        query = streaming_df.writeStream \
            .outputMode("append") \
            .format("memory") \
            .queryName(query_name) \
            .trigger(processingTime="10 seconds") \
            .start()
        
        print(f"\n‚úÖ Query '{query_name}' d√©marr√©e")
        print(f"Accessible via: spark.sql('SELECT * FROM {query_name}')")
        
        return query
    
    def monitor_query(self, query: StreamingQuery, duration_seconds: int = 60):
        """
        Monitorer une query streaming pendant un certain temps
        
        Args:
            query: StreamingQuery √† monitorer
            duration_seconds: Dur√©e de monitoring en secondes
        """
        print(f"\nüìä MONITORING DE LA QUERY '{query.name}'")
        print(f"Dur√©e: {duration_seconds} secondes")
        print("=" * 100)
        
        start_time = time.time()
        
        try:
            while time.time() - start_time < duration_seconds:
                if query.isActive:
                    status = query.status
                    print(f"\nStatut: {status}")
                    
                    progress = query.lastProgress
                    if progress:
                        print(f"Lignes trait√©es: {progress.get('numInputRows', 0)}")
                        print(f"Taux de traitement: {progress.get('processedRowsPerSecond', 0):.2f} rows/sec")
                    
                    time.sleep(10)
                else:
                    print("‚ö†Ô∏è  Query arr√™t√©e")
                    break
                    
        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è  Monitoring interrompu par l'utilisateur")
        
        print(f"\n‚úÖ Monitoring termin√©")
    
    def stop_all_queries(self):
        """Arr√™ter toutes les queries streaming actives"""
        active_queries = self.spark.streams.active
        
        if active_queries:
            print(f"\nüõë Arr√™t de {len(active_queries)} queries actives...")
            for query in active_queries:
                print(f"  - Arr√™t de '{query.name}'")
                query.stop()
            print("‚úÖ Toutes les queries sont arr√™t√©es")
        else:
            print("\n‚ÑπÔ∏è  Aucune query active")


def prepare_streaming_data(source_file: str, stream_dir: str, 
                          chunk_size: int = 1000, delay_seconds: int = 10):
    """
    Fonction utilitaire pour simuler le streaming en divisant
    un fichier de logs en plusieurs petits fichiers
    
    Args:
        source_file: Fichier de logs source
        stream_dir: R√©pertoire o√π placer les chunks
        chunk_size: Nombre de lignes par chunk
        delay_seconds: D√©lai entre les chunks (pour simulation)
    """
    import os
    import time
    
    print(f"\nüì¶ PR√âPARATION DES DONN√âES POUR STREAMING")
    print(f"Source: {source_file}")
    print(f"Destination: {stream_dir}")
    print(f"Taille des chunks: {chunk_size} lignes")
    print(f"D√©lai: {delay_seconds} secondes")
    print("=" * 100)
    
    # Cr√©er le r√©pertoire de streaming
    os.makedirs(stream_dir, exist_ok=True)
    
    # Lire le fichier source avec gestion d'encodage
    try:
        with open(source_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        # Essayer avec latin-1 si UTF-8 √©choue
        with open(source_file, 'r', encoding='latin-1') as f:
            lines = f.readlines()
    
    total_lines = len(lines)
    num_chunks = (total_lines + chunk_size - 1) // chunk_size
    
    print(f"\nTotal de lignes: {total_lines:,}")
    print(f"Nombre de chunks: {num_chunks}")
    
    # Diviser en chunks et √©crire avec d√©lai
    for i in range(num_chunks):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, total_lines)
        
        chunk_file = f"{stream_dir}/logs_chunk_{i:04d}.txt"
        
        with open(chunk_file, 'w') as f:
            f.writelines(lines[start_idx:end_idx])
        
        print(f"‚úÖ Chunk {i+1}/{num_chunks} √©crit: {chunk_file} ({end_idx - start_idx} lignes)")
        
        if i < num_chunks - 1:  # Ne pas attendre apr√®s le dernier chunk
            time.sleep(delay_seconds)
    
    print(f"\n‚úÖ Pr√©paration termin√©e: {num_chunks} chunks cr√©√©s")
