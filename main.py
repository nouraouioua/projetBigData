"""
Script principal pour l'analyse de logs web
Int√®gre toutes les fonctionnalit√©s: parsing, SQL, ML, GraphX, Streaming
"""

import sys
import argparse
from config.spark_config import create_spark_session, EXPORT_CONFIG
from src.log_parser import LogParser
from src.sql_analytics import SQLAnalytics
from src.anomaly_detection import AnomalyDetector
from src.graph_analyzer import GraphAnalyzer
from src.log_streamer import LogStreamer, prepare_streaming_data
from src.metrics_exporter import MetricsExporter, generate_grafana_metrics

def main():
    """Point d'entr√©e principal"""
    
    # Parser les arguments
    parser = argparse.ArgumentParser(description='Analyse de logs web avec Apache Spark')
    parser.add_argument('--mode', choices=['batch', 'streaming', 'all'], 
                       default='all', help='Mode d\'ex√©cution')
    parser.add_argument('--data', default='data/NASA_access_log_full.txt',
                       help='Fichier de logs √† analyser')
    parser.add_argument('--output', default='output',
                       help='R√©pertoire de sortie')
    parser.add_argument('--export-prometheus', action='store_true',
                       help='Exporter les m√©triques vers Prometheus')
    parser.add_argument('--k-clusters', type=int, default=5,
                       help='Nombre de clusters pour K-Means')
    
    args = parser.parse_args()
    
    print("=" * 100)
    print("üöÄ ANALYSE DE LOGS WEB AVEC APACHE SPARK")
    print("=" * 100)
    print(f"Mode: {args.mode}")
    print(f"Fichier: {args.data}")
    print(f"Output: {args.output}")
    print("=" * 100)
    
    # Cr√©er la session Spark
    spark = create_spark_session()
    
    if args.mode in ['batch', 'all']:
        run_batch_analysis(spark, args)
    
    if args.mode in ['streaming', 'all']:
        run_streaming_analysis(spark, args)
    
    if args.export_prometheus:
        run_prometheus_export(spark, args)
    
    print("\n" + "=" * 100)
    print("‚úÖ ANALYSE TERMIN√âE")
    print("=" * 100)
    
    # Garder Spark actif pour Prometheus
    if args.export_prometheus:
        print("\n‚è≥ Serveur Prometheus actif. Appuyez sur Ctrl+C pour arr√™ter...")
        try:
            import time
            while True:
                time.sleep(10)
        except KeyboardInterrupt:
            print("\nüõë Arr√™t du serveur...")
    
    spark.stop()


def run_batch_analysis(spark, args):
    """Ex√©cuter l'analyse batch compl√®te"""
    
    print("\n" + "=" * 100)
    print("üìä ANALYSE BATCH")
    print("=" * 100)
    
    # 1. PARSING
    print("\n### √âTAPE 1: PARSING DES LOGS ###")
    parser = LogParser(spark)
    logs_df = parser.parse_logs(args.data)
    parser.get_schema_info(logs_df)
    
    # Cache pour performance
    logs_df.cache()
    
    # 2. ANALYSES SQL
    print("\n### √âTAPE 2: ANALYSES SQL ###")
    analytics = SQLAnalytics(spark)
    kpis = analytics.generate_all_kpis(logs_df)
    
    # Sauvegarder les KPI en Parquet
    print("\nüíæ Sauvegarde des KPI en Parquet...")
    analytics.save_kpis_to_parquet(kpis, f"{args.output}/parquet")
    
    # 3. D√âTECTION D'ANOMALIES
    print("\n### √âTAPE 3: D√âTECTION D'ANOMALIES ###")
    detector = AnomalyDetector(spark)
    
    # K-Means clustering
    print("\n--- K-Means Clustering ---")
    anomalies_kmeans = detector.detect_anomalies_kmeans(logs_df, k=args.k_clusters)
    suspicious_ips = detector.get_suspicious_ips(anomalies_kmeans, top_n=50)
    
    # Analyse statistique
    print("\n--- Analyse Statistique ---")
    anomalies_stats = detector.detect_statistical_anomalies(logs_df, std_threshold=3.0)
    
    # Analyser le comportement des IP anormales
    detector.analyze_anomalous_behavior(logs_df, anomalies_kmeans)
    
    # Sauvegarder les anomalies
    print("\nüíæ Sauvegarde des anomalies...")
    suspicious_ips.write.mode('overwrite').parquet(f"{args.output}/parquet/suspicious_ips")
    
    # 4. ANALYSE DE GRAPHE
    print("\n### √âTAPE 4: ANALYSE DE GRAPHE (GraphX) ###")
    graph_analyzer = GraphAnalyzer(spark)
    
    # Construire le graphe
    graph = graph_analyzer.build_ip_url_graph(logs_df)
    
    # Analyses
    ip_connectivity = graph_analyzer.analyze_ip_connectivity(top_n=20)
    url_popularity = graph_analyzer.analyze_url_popularity(top_n=20)
    
    # D√©tecter les communaut√©s
    print("\n--- D√©tection de Communaut√©s ---")
    communities = graph_analyzer.find_communities()
    
    # Patterns suspects
    print("\n--- Patterns Suspects ---")
    suspicious_patterns = graph_analyzer.find_suspicious_patterns()
    
    # Export du graphe
    print("\nüíæ Export du graphe pour visualisation...")
    graph_analyzer.export_graph_for_visualization(f"{args.output}")
    
    # 5. EXPORT POUR GRAFANA
    print("\n### √âTAPE 5: EXPORT POUR GRAFANA ###")
    
    # Ajouter les anomalies aux KPI
    kpis['suspicious_ips'] = suspicious_ips
    kpis['ip_connectivity'] = ip_connectivity
    kpis['url_popularity'] = url_popularity
    
    generate_grafana_metrics(kpis, f"{args.output}/metrics")
    
    print("\n‚úÖ Analyse batch termin√©e!")


def run_streaming_analysis(spark, args):
    """Ex√©cuter l'analyse streaming"""
    
    print("\n" + "=" * 100)
    print("üåä ANALYSE STREAMING")
    print("=" * 100)
    
    # Pr√©parer les donn√©es de streaming (simuler)
    stream_dir = f"{args.output}/stream_input"
    
    print("\nüì¶ Pr√©paration des donn√©es de streaming...")
    print("(Diviser le fichier en chunks pour simuler le streaming)")
    
    import os
    if not os.path.exists(args.data):
        print(f"‚ùå Fichier {args.data} introuvable. T√©l√©chargez d'abord les donn√©es.")
        return
    
    # Cr√©er le r√©pertoire de streaming
    os.makedirs(stream_dir, exist_ok=True)
    
    # Initialiser le streamer
    streamer = LogStreamer(spark)
    
    # Cr√©er la source de streaming
    streaming_df = streamer.create_streaming_source(stream_dir)
    
    # Parser les logs en streaming
    parsed_stream = streamer.process_streaming_logs(streaming_df)
    
    # Agr√©ger les m√©triques
    metrics_stream = streamer.aggregate_streaming_metrics(parsed_stream)
    
    # √âcrire vers la console
    console_query = streamer.write_to_console(
        parsed_stream.select('ip', 'timestamp', 'method', 'url', 'status', 'bytes'),
        f"{args.output}/checkpoints",
        "streaming_logs"
    )
    
    # √âcrire les m√©triques vers Parquet
    metrics_query = streamer.write_to_parquet(
        metrics_stream,
        f"{args.output}/streaming_metrics",
        f"{args.output}/checkpoints",
        "streaming_metrics"
    )
    
    # √âcrire en m√©moire pour requ√™tes interactives
    memory_query = streamer.write_to_memory(parsed_stream, "live_logs")
    
    print("\n‚úÖ Queries de streaming d√©marr√©es!")
    print("\nQueries actives:")
    for query in spark.streams.active:
        print(f"  - {query.name}: {query.status}")
    
    # Lancer la simulation de streaming dans un thread s√©par√©
    print("\nüé¨ D√©marrage de la simulation de streaming...")
    print("   (Les chunks seront cr√©√©s toutes les 10 secondes)")
    
    import threading
    streaming_thread = threading.Thread(
        target=prepare_streaming_data,
        args=(args.data, stream_dir, 1000, 10)
    )
    streaming_thread.daemon = True
    streaming_thread.start()
    
    # Monitorer pendant 2 minutes
    streamer.monitor_query(console_query, duration_seconds=120)
    
    # Arr√™ter toutes les queries
    streamer.stop_all_queries()
    
    print("\n‚úÖ Analyse streaming termin√©e!")


def run_prometheus_export(spark, args):
    """Exporter les m√©triques vers Prometheus"""
    
    print("\n" + "=" * 100)
    print("üì° EXPORT PROMETHEUS")
    print("=" * 100)
    
    # Parser les logs
    parser = LogParser(spark)
    logs_df = parser.parse_logs(args.data)
    
    # Cr√©er l'exporteur
    exporter = MetricsExporter(port=8000)
    exporter.start_server()
    
    # Mettre √† jour les m√©triques
    print("\nüìä Mise √† jour des m√©triques...")
    exporter.update_metrics_from_dataframe(logs_df)
    
    # D√©tecter les anomalies et mettre √† jour
    detector = AnomalyDetector(spark)
    anomalies = detector.detect_anomalies_kmeans(logs_df, k=args.k_clusters)
    exporter.update_anomaly_metrics(anomalies)
    
    print("\n‚úÖ M√©triques export√©es vers Prometheus")
    print(f"   URL: http://localhost:8000/metrics")


if __name__ == '__main__':
    main()
